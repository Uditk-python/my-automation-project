# Python Automation Projects 🚀

This repository contains various Python automation scripts created as part of my structured learning journey.
# 🕸️ Week 1:Basic automation - Python Automation Journey
## 📌 Projects Included:
- *Day 1: OS Module Basics 🖥️: starting with os module basic commands
- *Day 2: File Handling 📂: modifying ,creating new files using python
- *Day 3: Shutil for File Management 🔄:using shutil to move or copy and delete
- *Day 4: CSV & Pandas Data Handling 📊:using panda to conver csv to excel
- *Day 5: Folder Organization Script 📁: cleaning a messy folder using python
- *Day 6: File & Folder Deletion Automation ❌:backup project
- *Day 7: GitHub Upload 🚀
# 🕸️ Week 2: Web Scraping & API Automation – Python Automation Journey

This week focuses on mastering data extraction using **BeautifulSoup**, **Selenium**, and **APIs**. The goal was to build functional, real-world automation tools for scraping, form filling, and fetching online data.
- *Day 8: Web Scraping with BeautifulSoup
---- Scraped headlines from **Hindustan Times**.
---- Used `requests`, `BeautifulSoup`, and saved results to a `.txt` file.
- *Day 9: CSS Selectors & XPath Practice
----- Practiced extracting elements using:
----- Tag selectors (`h1`, `h2`, etc.)
----- CSS classes and attributes
----- XPath queries
- *Day 10: Selenium Login Automation
----- Automated login to **SauceDemo** website.
----- Checked for incorrect login error or success message.
- *Day 11: Form Filling with Selenium
----- Auto-filled a practice form on [Techlistic Selenium Form](https://www.techlistic.com/p/selenium-practice-form.html).
----- Selected radio buttons, checkboxes, file upload, and dropdowns.
- *Day 12: API Data Extraction
----- Used **OpenWeatherMap API** to fetch real-time weather.
----- Extracted temperature and weather description for user-entered city.
- *Day 13: Amazon Watch Scraper (Pagination)
----- Scraped product names, prices, images, and links from **Amazon** (multiple pages).
----- Saved to `.csv` and converted to `.xlsx`.


- Gained strong hands-on experience in:
-- Scraping both static and dynamic websites
-- Automating forms
-- Using REST APIs
-- Managing data outputs in TXT, CSV, Excel

🔗 **Next Step**: Begin freelance research and portfolio building (Day 14)

📌 **GitHub Repo Tips**:
- Upload all scripts in a `Week2_WebScraping` folder.
- Include this README.md.
- Use clear filenames and comments in your code.

# 🔧 How to Run the Scripts:
1. Clone this repository:
   ```bash
   git clone https://github.com/Uditk-python/Python-Automation.git
✅ Contribution:
   Feel free to suggest improvements or add new automation scripts. 
=======
2.✅ Contribution:
    Feel free to suggest improvements or add new automation scripts.
